{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b982e28-0df0-41b7-b35e-64c862963df2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa48058-faa3-47e6-8050-fc92b7f9f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3360d6-0c9c-4de8-af51-db05a240aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_zip_is_which_data_source(input_data_folder):\n",
    "    '''\n",
    "    Function that searches the input data folder for zip files and determines \n",
    "    which zipfiles contain the data needed for this task. The function also \n",
    "    identifies the type of data/data source for each of those zipfiles. \n",
    "    For example: 'texas_inrix_npmrds_15min(1).zip' contains the 'NPMRDS from \n",
    "    INRIX (Passenger vehicles)' data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_paths_dict: DICT\n",
    "        Dictionary that contains information about where the files for each \n",
    "        data source is located. It should be structured as follows:\n",
    "            {'data_origin_1':{'zip_file':'zip_file_1_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_1.csv'},\n",
    "             'data_origin_2':{'zip_file':'zip_file_2_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_2.csv'},\n",
    "             ...}\n",
    "    '''\n",
    "    \n",
    "    # Dictionary that is used to match data origin to the RegEx string\n",
    "    dict_for_origin_match = {\n",
    "        'inrix':\n",
    "            '.*INRIX TMC.*',\n",
    "        'npmrds_from_inrix_pass_vehicles':\n",
    "            '.*NPMRDS from INRIX \\(Passenger vehicles\\).*',\n",
    "        'npmrds_from_inrix_trucks':\n",
    "            '.*NPMRDS from INRIX \\(Trucks\\).*',\n",
    "        'npmrds_from_inrix_trucks_and_passveh':\n",
    "            '.*NPMRDS from INRIX \\(Trucks and passenger vehicles\\).*'}\n",
    "    \n",
    "    \n",
    "    # Find all zip files in input folder\n",
    "    folder_files = [f for f in os.listdir(input_data_folder) if \n",
    "                    os.path.isfile(os.path.join(input_data_folder, f))]\n",
    "    \n",
    "    # Filter only zipfiles\n",
    "    folder_zips = list(filter(re.compile(\".*zip$\").match, folder_files))\n",
    "    \n",
    "    # Dictionary that will store the output\n",
    "    data_paths_dict = {}\n",
    "    \n",
    "    # Looping over zipfiles\n",
    "    for this_zip_file in folder_zips:\n",
    "        with zipfile.ZipFile(os.path.join(input_data_folder, \n",
    "                                          this_zip_file)) as this_zip:\n",
    "            \n",
    "            # Checking if this is a data extract from RITIS' massive data downloader\n",
    "            files_in_zip = this_zip.namelist()\n",
    "            if 'Contents.txt' in files_in_zip:\n",
    "                \n",
    "                #Extracting the name of the raw data CSV file inside this zipfile\n",
    "                raw_data_file = [this_file for this_file in files_in_zip if \n",
    "                                 this_file !='Contents.txt' and \n",
    "                                 this_file !='TMC_Identification.csv'][0]\n",
    "                \n",
    "                # Performing a RegEx search to find which data source this \n",
    "                # zipfile originally came from\n",
    "                with this_zip.open('Contents.txt','r') as content_file:\n",
    "                    this_content = content_file.readline().decode('utf-8')\n",
    "                    for this_data_origin, this_regex_string in (\n",
    "                            dict_for_origin_match.items()):\n",
    "                        regex_search = re.match(this_regex_string,this_content)\n",
    "                        if regex_search:\n",
    "                            data_paths_dict[this_data_origin] = {\n",
    "                                'zip_file':os.path.join(input_data_folder,\n",
    "                                                        this_zip_file),\n",
    "                                'raw_data_file':raw_data_file}\n",
    "    return data_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67106b41-a9f5-4f1e-88f0-3b9061169896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_get_specific_road_segments(data_origin,\n",
    "                                        raw_data_zipfile,\n",
    "                                        raw_data_filename_in_zip,\n",
    "                                        road_str,chunk_size,\n",
    "                                        raw_data_chunks=None,\n",
    "                                        tmc_data_parts=None):\n",
    "    \"\"\"\n",
    "    Function used to read in raw speed and TMC segment data. This function will \n",
    "    likely be called multiple times because of the several different sources of \n",
    "    input files. For example: INRIX, NPMRDS from INRIX, etc.\n",
    "    \n",
    "    INPUT VARIABLES:\n",
    "    ----------------\n",
    "    data_origin: STR \n",
    "       String that characterizes the origin of the data. Sample values: \n",
    "       'inrix','npmrds_from_inrix_trucks', 'npmrds_from_pass_vehicles'\n",
    "    raw_data_zipfile: STR\n",
    "        String that contains the file/folder location of the zipfile to be read\n",
    "    raw_data_filename_in_zip: STR\n",
    "        String that contains the filename of the raw data inside the zipfile\n",
    "    road_str: STR used to filter road segments based on their names. The TMC \n",
    "        segments will be filtered based on whether or not the 'road' column \n",
    "        contains this string. To get the entire dataset back, just use an \n",
    "        empty string ('').\n",
    "    chunk_size: INT\n",
    "        Integer used to determine number of rows read at a time by Pandas' \n",
    "        read_csv method.\n",
    "    raw_data_chunks: LIST \n",
    "        List containing the several chunks of input files thus far.\n",
    "        The first time this function is called, this should just be an empty list.\n",
    "    tmc_data_parts: LIST \n",
    "        List containing the several tmc_data inputs from the multiple times this \n",
    "        function is called.\n",
    "                        \n",
    "    OUTPUT:\n",
    "    -------\n",
    "    output_dict : DICT\n",
    "        Dictionary that contains two values: 'raw_data_chunks' and 'tmc_data_parts':\n",
    "        raw_data_chunks: LIST \n",
    "            List of pd.DataFrames that contain the several chunks of all the \n",
    "            input  data-files, including the chunks created in the current \n",
    "            execution of this method. \n",
    "            Note: It is expected that these chunks will later be concatenated \n",
    "            into one large DataFrame afterwards.\n",
    "        tmc_data_parts: LIST \n",
    "            List of pd.DataFrames containing the analogous TMC data \n",
    "            (i.e., the data in the \"TMC_Identification.csv\" files).\n",
    "    \"\"\" \n",
    "    \n",
    "    if not raw_data_chunks:\n",
    "        raw_data_chunks = []\n",
    "    if not tmc_data_parts:\n",
    "        tmc_data_parts = []\n",
    "    \n",
    "    # Opening the zipfile\n",
    "    with zipfile.ZipFile(raw_data_zipfile) as this_zip:\n",
    "        \n",
    "        # Reading in the TMC data from the zipfile\n",
    "        with this_zip.open('TMC_Identification.csv','r') as tmc_data_file:\n",
    "            tmc_data = pd.read_csv(tmc_data_file, low_memory=False)\n",
    "    \n",
    "        # Adding extra column about data origin and storing the final result\n",
    "        tmc_data['data_origin'] = data_origin\n",
    "        tmc_data['road'] = tmc_data['road'].fillna('')\n",
    "        \n",
    "        # Fixing column names\n",
    "        tmc_data = tmc_data.rename({'tmc':'tmc_code',\n",
    "                                    'intersection':'intersection_',\n",
    "                                    'state':'state_',\n",
    "                                    'type':'type_'},axis=1)\n",
    "        \n",
    "        # Querying main searched road\n",
    "        tmc_data = tmc_data.query(f'road.str.contains(\"{road_str}\")', \n",
    "                                  engine='python')\n",
    "        \n",
    "        # Sometimes, this DataFrame has multiple rows for the same TMC. \n",
    "        # This step is taken to de-duplicate the TMCs data.\n",
    "        tmc_data = (tmc_data\n",
    "                    .sort_values(by=['tmc_code','active_end_date'])\n",
    "                    .reset_index(drop=True))\n",
    "        tmc_data = tmc_data.groupby('tmc_code').last().reset_index()\n",
    "        tmc_data_parts.append(tmc_data.copy())\n",
    "        \n",
    "        # Subset of the TMC data with only the relevant columns\n",
    "        tmc_data_sub = tmc_data[['tmc_code','road','data_origin']]\n",
    "        \n",
    "        # Reading in the raw data in chunks and only keeping segments that \n",
    "        # are related to the main searched road\n",
    "        with this_zip.open(raw_data_filename_in_zip,'r') as tmc_data_file:\n",
    "            with pd.read_csv(tmc_data_file, \n",
    "                             chunksize=chunk_size, \n",
    "                             dtype={'tmc_code':'str'}) as reader:\n",
    "                for raw_data in reader:\n",
    "                    raw_data = raw_data.merge(tmc_data_sub, \n",
    "                                              how='left', on='tmc_code')\n",
    "                    raw_data = raw_data.loc[raw_data.road.notnull()]\n",
    "                    raw_data_chunks.append(raw_data.copy())\n",
    "    \n",
    "    # Since we need to return more than one output, the multiple outputs have \n",
    "    # been added to a dictionary.\n",
    "    output_dict = {'raw_data_chunks':raw_data_chunks,\n",
    "                   'tmc_data_parts':tmc_data_parts}\n",
    "     \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb545c-1b38-4a25-9ce7-113760147bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_set_of_raw_data(input_data_folder,road_str,chunk_size,data_origin):\n",
    "    '''\n",
    "    Looks into the input folder and reads in the raw data contained in only \n",
    "    one of the zipfiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "    road_str: STR \n",
    "        String used to filter road segments based on their names. This is also \n",
    "        referred to as \"the main searched road\" in other places of this script.\n",
    "        The TMC segments will be filtered based on whether or not the 'road' \n",
    "        column contains this string. To get the entire dataset back, just use \n",
    "        an empty string ('').\n",
    "    chunk_size : INT\n",
    "        Integer that defines the chunk size for Pandas' `read_csv` method.\n",
    "    data_origin: STR \n",
    "       String that characterizes the origin of the data. Sample values: \n",
    "       'inrix','npmrds_from_inrix_trucks', 'npmrds_from_pass_vehicles'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_dict: DICT\n",
    "        Dictionary with two entries: \"main_data\" and \"main_tmc_data\".\n",
    "        main_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the actual raw speed data for the main \n",
    "            searched road\n",
    "        main_tmc_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the associated TMC data for all the \n",
    "            TMC segments on the main searched road\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Searching the input folder for zipfiles and determining where the relevant\n",
    "    # raw data files are. This function also tells you which \"data_origin\" is \n",
    "    # associated with each of the zipfiles.\n",
    "    data_paths_dict = which_zip_is_which_data_source(input_data_folder)\n",
    "    \n",
    "    \n",
    "    # This is an empty list that will store the DataFrame chunks from reading \n",
    "    # in the raw speed data.\n",
    "    raw_data_chunks = []\n",
    "    \n",
    "    # This is an empty list that will store the DataFrames containing the TMC-\n",
    "    # segment  link data that is associated with each data source (i.e., the \n",
    "    # data in the \"TMC_Identification.csv\" files)\n",
    "    tmc_data_parts = []\n",
    "    \n",
    "    \n",
    "    raw_data_zipfile = data_paths_dict[data_origin]['zip_file']\n",
    "    raw_data_filename_in_zip = data_paths_dict[data_origin]['raw_data_file']\n",
    "    results_dict = read_csv_get_specific_road_segments(\n",
    "                             data_origin=data_origin,\n",
    "                             raw_data_zipfile=raw_data_zipfile,\n",
    "                             raw_data_filename_in_zip=raw_data_filename_in_zip,\n",
    "                             road_str=road_str,\n",
    "                             chunk_size=chunk_size,\n",
    "                             raw_data_chunks=raw_data_chunks,\n",
    "                             tmc_data_parts=tmc_data_parts)\n",
    "    raw_data_chunks = results_dict['raw_data_chunks']\n",
    "    tmc_data_parts = results_dict['tmc_data_parts']\n",
    "    \n",
    "    # Concatenating all raw data chunks into one single DataFrame\n",
    "    main_data = pd.concat(raw_data_chunks, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Making sure there are no duplicates. If there are, they are averaged out.\n",
    "    #main_data = main_data.groupby(['data_origin','tmc_code','measurement_tstamp']).mean().reset_index()\n",
    "    main_data = main_data.drop_duplicates(subset=['data_origin','tmc_code','measurement_tstamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Dropping observations/rows where there is no speed data. \n",
    "    # This whole process is geared towards finding average (and percentiles) of\n",
    "    # speeds. If the data point provides us with no speed info, there is \n",
    "    # nothing else we can use that data point for.\n",
    "    main_data = main_data.loc[main_data['speed'].notna()].reset_index(drop=True)\n",
    "    \n",
    "    # Concatenating all TMC data parts into one single DataFrame\n",
    "    main_tmc_data = pd.concat(tmc_data_parts, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Since we need to return more than one output, the multiple outputs have \n",
    "    # been added to a dictionary.\n",
    "    output_dict = {'main_data':main_data,\n",
    "                   'main_tmc_data':main_tmc_data}\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a6be1-23a9-49f0-98b1-ad8e4ea1047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_of_raw_data(input_data_folder,road_str,chunk_size):\n",
    "    '''\n",
    "    Looks into the input folder and reads all of the zipfiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "    road_str: STR \n",
    "        String used to filter road segments based on their names. This is also \n",
    "        referred to as \"the main searched road\" in other places of this script.\n",
    "        The TMC segments will be filtered based on whether or not the 'road' \n",
    "        column contains this string. To get the entire dataset back, just use \n",
    "        an empty string ('').\n",
    "    chunk_size : INT\n",
    "        Integer that defines the chunk size for Pandas' `read_csv` method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_dict: DICT\n",
    "        Dictionary with two entries: \"main_data\" and \"main_tmc_data\".\n",
    "        main_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the actual raw speed data for the main \n",
    "            searched road\n",
    "        main_tmc_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the associated TMC data for all the \n",
    "            TMC segments on the main searched road            \n",
    "\n",
    "    '''\n",
    "    # Searching the input folder for zipfiles and determining where the relevant\n",
    "    # raw data files are. This function also tells you which \"data_origin\" is \n",
    "    # associated with each of the zipfiles.\n",
    "    data_paths_dict = which_zip_is_which_data_source(input_data_folder)\n",
    "    \n",
    "    \n",
    "    # This is an empty list that will store the DataFrames containing the raw \n",
    "    # speed data.\n",
    "    main_data_parts = []\n",
    "    \n",
    "    # This is an empty list that will store the DataFrames containing the TMC-\n",
    "    # segment link data that is associated with each data source (i.e., the \n",
    "    # data in the \"TMC_Identification.csv\" files)\n",
    "    tmc_data_parts = []\n",
    "    \n",
    "    # Actually running the batch input process\n",
    "    for i,this_data_origin in enumerate(data_paths_dict):\n",
    "        this_batch_results = read_one_set_of_raw_data(input_data_folder,\n",
    "                                                      road_str,\n",
    "                                                      chunk_size,\n",
    "                                                      this_data_origin)\n",
    "        main_data_parts.append(this_batch_results['main_data'])\n",
    "        tmc_data_parts.append(this_batch_results['main_tmc_data'])\n",
    "    \n",
    "    # Concatenating all main data parts into one single DataFrame\n",
    "    main_data = pd.concat(main_data_parts, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Making sure there are no duplicates. If there are, they are averaged out.\n",
    "    #main_data = main_data.groupby(['data_origin','tmc_code','measurement_tstamp']).mean().reset_index()\n",
    "    main_data = main_data.drop_duplicates(subset=['data_origin','tmc_code','measurement_tstamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Dropping observations/rows where there is no speed data. \n",
    "    # This whole process is geared towards finding average (and percentiles) of\n",
    "    # speeds. If the data point provides us with no speed info, there is \n",
    "    # nothing else we can use that data point for.\n",
    "    main_data = main_data.loc[main_data['speed'].notna()].reset_index(drop=True)\n",
    "    \n",
    "    # Concatenating all TMC data parts into one single DataFrame\n",
    "    main_tmc_data = pd.concat(tmc_data_parts, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Since we need to return more than one output, the multiple outputs have \n",
    "    # been added to a dictionary.\n",
    "    output_dict = {'main_data':main_data,\n",
    "                   'main_tmc_data':main_tmc_data}\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895da6a-1be3-45e9-8152-21ed76402b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_datetime_columns(main_data):\n",
    "    '''\n",
    "    Generates an actual datetime column in the \"main_data\" DataFrame by parsing \n",
    "    the text-based timestamp column. Also extracts day-of-week and time info \n",
    "    into separate columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        The pandas DataFrame that contains all the raw data from the RITIS\n",
    "        website (INRIX/NPMRDS speeds)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    main_data : pd.DataFrame\n",
    "        The same DataFrame as the input, except that now, the DataFrame has a \n",
    "        few new datetime-related columns. Namely:\n",
    "            -day_of_week: indicates the row's day of the week as a number from \n",
    "                0 (Monday) to 6 (Sunday)\n",
    "            -day_of_week_str: indicates the row's day of the week as a string\n",
    "                of text\n",
    "            -time: indicates the row's TIME (without date)\n",
    "\n",
    "    '''\n",
    "    # Transforming STRING timestamp into an actual datetime format\n",
    "    main_data['measurement_tstamp'] = pd.to_datetime(main_data['measurement_tstamp'])\n",
    "    \n",
    "    # Extracting day-of-week data and making it more readable.\n",
    "    # Monday=0, Sunday=6\n",
    "    main_data['day_of_week'] = main_data.measurement_tstamp.dt.day_of_week\n",
    "    main_data['day_of_week_str'] = (main_data['day_of_week']\n",
    "                                    .apply(lambda x: {0:'0 - Monday',\n",
    "                                                      1:'1 - Tuesday',\n",
    "                                                      2:'2 - Wednesday',\n",
    "                                                      3:'3 - Thursday',\n",
    "                                                      4:'4 - Friday',\n",
    "                                                      5:'5 - Saturday',\n",
    "                                                      6:'6 - Sunday'}[x]))\n",
    "    \n",
    "    \n",
    "    # Extracting day-of-year data\n",
    "    main_data['day_of_year'] = main_data.measurement_tstamp.dt.day_of_year\n",
    "    \n",
    "    # Extracting the time value, which was coded originally in 15 minute intervals\n",
    "    main_data['time'] = main_data.measurement_tstamp.dt.time\n",
    "    \n",
    "    return main_data\n",
    "\n",
    "class time_slot():\n",
    "    '''\n",
    "    Class that is used to label the observations in the `main_data` DataFrame \n",
    "    (that contains all the raw data from the RITIS speeds database) according \n",
    "    to the time of day. For example: am_peak, pm_peak, etc.\n",
    "    ''' \n",
    "    def __init__(self,time_start,time_end,include_start, include_end, \n",
    "                 inside_outside,slot_name):\n",
    "        '''\n",
    "        Instantiates `time_slot`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        time_start : datetime.time\n",
    "            Start time of the time slot\n",
    "        time_end : datetime.time\n",
    "            End time of the time slot\n",
    "        include_start : BOOL\n",
    "            Indicates whether to use >= or just > for time_start\n",
    "        include_end : BOOL\n",
    "            Indicates whether to use <= or just < for time_end\n",
    "        inside_outside : STR\n",
    "            Indicates whether the time slot refers to the time inside or outside\n",
    "            of the start and end times. To be more specific:\n",
    "                If inside_outside==\"inside\", then the time slot refers to the \n",
    "                time AFTER time_start but BEFORE time_end. \n",
    "                If inside_outside==\"outside\" , then the time slot refers to the \n",
    "                time BEFORE time_start but AFTER time_end (e.g.: before 6am and \n",
    "                after 10pm). \n",
    "        slot_name : STR\n",
    "            Describes the name of the time slot. Typical names include \"am_peak\",\n",
    "            \"pm_peak\", \"off_peak\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The newly-created instance of this class.\n",
    "\n",
    "        '''\n",
    "        self.time_start     = time_start\n",
    "        self.time_end       = time_end\n",
    "        self.include_start  = include_start\n",
    "        self.include_end    = include_end\n",
    "        self.inside_outside = inside_outside\n",
    "        self.slot_name      = slot_name\n",
    "        \n",
    "    \n",
    "    def get_filter(self, main_data):\n",
    "        '''\n",
    "        Gets the filter/mask that indicates which of the INRIX observations belong\n",
    "        to this specific time slot.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ts_filter : pd.Series (bool)\n",
    "            An array of BOOL variables that indicates whether or not each \n",
    "            observation belongs to this specific time slot. The array has length\n",
    "            equal to the number of rows in main_data.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            return self.ts_filter\n",
    "        except: \n",
    "            if self.inside_outside == 'inside':\n",
    "                this_filter = ((self.time_start < main_data['time']) & \n",
    "                               (main_data['time'] < self.time_end))\n",
    "            elif self.inside_outside == 'outside':\n",
    "                this_filter = ((main_data['time'] < self.time_start) | \n",
    "                               (self.time_end < main_data['time']))\n",
    "            if self.include_start:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['time'] == self.time_start))\n",
    "            if self.include_end:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['time'] == self.time_end))\n",
    "            self.ts_filter = this_filter\n",
    "            return self.ts_filter\n",
    "        \n",
    "    def add_time_slot_data_to_main_data(self, main_data):\n",
    "        '''\n",
    "        Adds the 'time_slot' column to the data and applies `time_slot`'s name\n",
    "        to the appropriate rows.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        main_data : pd.DataFrame\n",
    "            The same DataFrame as was passed in the input. The only difference \n",
    "            is that now, the `time_slot`'s name was applied to the rows that \n",
    "            fall within the `time_slot`'s filter.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            main_data.loc[self.get_filter,'time_slot'] = self.slot_name\n",
    "            return main_data\n",
    "        except:\n",
    "            main_data['time_slot'] = np.nan\n",
    "            main_data.loc[self.get_filter,'time_slot'] = self.slot_name\n",
    "            return main_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541a153-e209-423d-b7ad-62689461a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class day_of_year_window():\n",
    "    '''\n",
    "    Class that is used to label the observations in the `main_data` DataFrame \n",
    "    (that contains all the raw data from the RITIS speeds database) according \n",
    "    to the day of the year. For example: some analyses require only data between \n",
    "    September and October.\n",
    "    ''' \n",
    "    def __init__(self,start_date,end_date,include_start, include_end, \n",
    "                 inside_outside,window_name):\n",
    "        '''\n",
    "        Instantiates `day_of_year_window`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : INT\n",
    "            Integer that indicates the day_of_year for the window's start date\n",
    "        end_date : INT\n",
    "            Integer that indicates the day_of_year for the window's end date\n",
    "        include_start : BOOL\n",
    "            Indicates whether to use >= or just > for start_date\n",
    "        include_end : BOOL\n",
    "            Indicates whether to use <= or just < for end_date\n",
    "        inside_outside : STR\n",
    "            Indicates whether the window refers to the days inside or outside\n",
    "            of the start and end dates. To be more specific:\n",
    "                If inside_outside==\"inside\", then the window refers to the \n",
    "                time AFTER start_date but BEFORE end_date. \n",
    "                If inside_outside==\"outside\" , then the time slot refers to the \n",
    "                time BEFORE start_date but AFTER end_date (e.g.: Before January\n",
    "                25th and after November 12th). \n",
    "        window_name : STR\n",
    "            Describes the name of the window.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The newly-created instance of this class.\n",
    "\n",
    "        '''\n",
    "        self.start_date     = start_date\n",
    "        self.end_date       = end_date\n",
    "        self.include_start  = include_start\n",
    "        self.include_end    = include_end\n",
    "        self.inside_outside = inside_outside\n",
    "        self.window_name    = window_name\n",
    "        \n",
    "    \n",
    "    def get_filter(self, main_data):\n",
    "        '''\n",
    "        Gets the filter/mask that indicates which of the INRIX observations belong\n",
    "        to this specific date window.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        doy_filter : pd.Series (bool)\n",
    "            An array of BOOL variables that indicates whether or not each \n",
    "            observation belongs to this specific window. The array has length\n",
    "            equal to the number of rows in main_data.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            return self.doy_filter\n",
    "        except: \n",
    "            if self.inside_outside == 'inside':\n",
    "                this_filter = ((self.start_date < main_data['day_of_year']) &\n",
    "                               (main_data['day_of_year'] < self.end_date))\n",
    "            elif self.inside_outside == 'outside':\n",
    "                this_filter = ((main_data['day_of_year'] < self.start_date) | \n",
    "                               (self.end_date < main_data['day_of_year']))\n",
    "            if self.include_start:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['day_of_year'] == self.start_date))\n",
    "            if self.include_end:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['day_of_year'] == self.end_date))\n",
    "            self.doy_filter = this_filter\n",
    "            return self.doy_filter\n",
    "        \n",
    "    def add_window_data_to_main_data(self, main_data):\n",
    "        '''\n",
    "        Adds the 'date_window' column to the data and applies the \n",
    "        `day_of_year_window`'s name to the appropriate rows.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        main_data : pd.DataFrame\n",
    "            The same DataFrame as was passed in the input. The only difference \n",
    "            is that now, the `day_of_year_window`'s name was applied to the rows \n",
    "            that fall within the `day_of_year_window`'s filter.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            main_data.loc[self.get_filter,'date_window'] = self.window_name\n",
    "            return main_data\n",
    "        except:\n",
    "            main_data['date_window'] = np.nan\n",
    "            main_data.loc[self.get_filter,'date_window'] = self.window_name\n",
    "            return main_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd334c-ecdb-46c3-a150-26902fc9a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summaries(grouped_data):\n",
    "    '''\n",
    "    Calculates all the important summaries for means and percentiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grouped_data : DataFrameGroupBy object\n",
    "        DataFrame that was filtered down and grouped using the `groupby` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grouped_data_summaries : pd.DataFrame\n",
    "        Pandas DataFrame containing all of the summary results: means and \n",
    "        percentiles for speed and travel time.\n",
    "\n",
    "    '''\n",
    "    # Determining which column name to use: minutes or seconds\n",
    "    if 'travel_time_minutes' in grouped_data.head().columns:\n",
    "        tt_col = 'travel_time_minutes'\n",
    "    else:\n",
    "        tt_col = 'travel_time_seconds'\n",
    "    \n",
    "    grouped_data_summaries = grouped_data.agg(\n",
    "        count_obs = pd.NamedAgg(column='measurement_tstamp', aggfunc='count'),\n",
    "                                                \n",
    "        speed_avg = pd.NamedAgg(column='speed', aggfunc=('mean')),\n",
    "        speed_01p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q =  1))),\n",
    "        speed_05p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q =  5))),\n",
    "        speed_15p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 15))),\n",
    "        speed_20p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 20))),\n",
    "        speed_50p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 50))),\n",
    "        speed_80p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 80))),\n",
    "        speed_85p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 85))),\n",
    "        speed_95p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 95))),\n",
    "        speed_99p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 99))),\n",
    "        \n",
    "        ttime_avg = pd.NamedAgg(column=tt_col, aggfunc=('mean')),\n",
    "        ttime_01p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q =  1))),\n",
    "        ttime_05p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q =  5))),\n",
    "        ttime_15p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 15))),\n",
    "        ttime_20p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 20))),\n",
    "        ttime_50p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 50))),\n",
    "        ttime_80p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 80))),\n",
    "        ttime_85p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 85))),\n",
    "        ttime_95p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 95))),\n",
    "        ttime_99p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 99))),\n",
    "        \n",
    "        )\n",
    "    return grouped_data_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45747bc2-8524-4065-bf5f-58b9127c41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summaries_pipeline(main_data,\n",
    "                            summary_name,\n",
    "                            summary_filter,\n",
    "                            grouping_columns):\n",
    "    '''\n",
    "    This function simplifies and standardizes the process of calculating \n",
    "    summaries from the raw data. The user needs to tell the function what\n",
    "    rows are to be kept, what columns will be used to group the data and the \n",
    "    name of this particular summary.\n",
    "    This function then returns the newly-calculated summarized data containing\n",
    "    a new column called \"summary_type\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        The pandas DataFrame that contains all the raw data from the RITIS\n",
    "        website (INRIX/NPMRDS speeds).\n",
    "    summary_name : str\n",
    "        String that describes this summary type. After the `main_data` is \n",
    "        summarized, a new column called \"summary_type\" will be generated. \n",
    "        This new column will contain the text stored in the `summary_name`\n",
    "        variable.\n",
    "    summary_filter : np.array\n",
    "        Array containing only boolean values (False/True). This indicates which\n",
    "        rows from the `main_data` should be used in the calculation of these \n",
    "        summaries.\n",
    "    grouping_columns : list\n",
    "        List of column names that will be used to group the `main_data`\n",
    "        dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summarized_data : pd.DataFrame\n",
    "        Pandas DataFrame containing all of the summary results: means and \n",
    "        percentiles for speed and travel time.\n",
    "\n",
    "    '''\n",
    "    grouped_data = main_data.loc[summary_filter].groupby(grouping_columns)\n",
    "    \n",
    "    summarized_data = calc_summaries(grouped_data)\n",
    "    \n",
    "    summarized_data['summary_type'] = summary_name\n",
    "    \n",
    "    return summarized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ea7ac-8bd3-4cf5-b495-c2b8453ea4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npmrds_geodata(npmrds_geodata_path):\n",
    "    '''\n",
    "    Reads in the shapefile associated with the NPMRDS data. Typically, this \n",
    "    file is just called \"Texas.shp\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    npmrds_geodata_path : STR\n",
    "        String describing the full path to the \".shp\" file on disk of where the \n",
    "        NPMRDS data can be found. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npmrds_geodata : gpd.GeoDataFrame\n",
    "        The GeoDataFrame with the actual geodata from the NPMRDS shapefile.\n",
    "    '''\n",
    "    \n",
    "    npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename({'Tmc':'tmc_code'},\n",
    "                                                               axis=1)\n",
    "    \n",
    "    return npmrds_geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2199e4-c918-4692-b680-d0d176d7e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link(df_row):\n",
    "    '''\n",
    "    Function that creates a simplified link geometry (straight line) using the \n",
    "    start/end long/lat data from the original INRIX main data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_row : pd.Series\n",
    "        One row of the `main_data_summaries` DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    row_line : shapely.LineString\n",
    "        Geometric feature created using the lat/lon data from in the input row\n",
    "\n",
    "    '''\n",
    "    start_pt = shapely.geometry.Point([df_row['start_longitude'], \n",
    "                                       df_row['start_latitude']])\n",
    "    end_pt = shapely.geometry.Point([df_row['end_longitude'], \n",
    "                                     df_row['end_latitude']])\n",
    "    row_line = shapely.geometry.LineString([start_pt,end_pt])\n",
    "    return row_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaafb6c-072a-44d8-9441-1538ebf90b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometries_to_summaries(summarized_data, \n",
    "                                main_tmc_data,\n",
    "                                npmrds_geodata_path):\n",
    "    '''\n",
    "    Adds a column called \"geom_final\" to the dataset. This new column contains \n",
    "    a geometry for each row in the summary dataset. This geometry is generated \n",
    "    in one of two different ways, in the following priority:\n",
    "        -Look in the NPMRDS shapefile to try and find a link with matching TMC\n",
    "        -If we can't find one, we just draw a straight line from the lat/lon\n",
    "            data that is found in the TMC_Identification.csv files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summarized_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the speed summaries. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the \n",
    "        TMC segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "    npmrds_geodata_path : STR\n",
    "        String that identifies where to find the NPMRDS shapefile. Needs to \n",
    "        point to the \".shp\" file. Typically, this file is just called \"Texas.shp\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    summarized_data_with_geoms : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing the geometries associated with each link.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename({'Tmc':'tmc_code'},\n",
    "                                                               axis=1)\n",
    "    \n",
    "    # Merging summaries with NPMRDS geometries\n",
    "    summarized_data_with_geoms = summarized_data.merge(\n",
    "        npmrds_geodata[['tmc_code','geometry']].to_crs('epsg:4326'), \n",
    "        how='left', \n",
    "        on='tmc_code')\n",
    "    \n",
    "#     # Applying the \"make_link\" function to generate the simplified geometries\n",
    "#     temp_geoms = (summarized_data_with_geoms\n",
    "#                   .drop_duplicates(subset=['data_origin','tmc_code'])\n",
    "#                   .sort_values(by=['data_origin','tmc_code'])\n",
    "#                   .reset_index(drop=True))\n",
    "    \n",
    "#     temp_geoms = temp_geoms.merge(main_tmc_data[['tmc_code',\n",
    "#                                                  'data_origin',\n",
    "#                                                  'start_latitude',\n",
    "#                                                  'start_longitude',\n",
    "#                                                  'end_latitude',\n",
    "#                                                  'end_longitude']], \n",
    "#                                   how='left', \n",
    "#                                   on=['data_origin','tmc_code'])\n",
    "    \n",
    "#     temp_geoms['geometry_simplified'] = temp_geoms.apply(make_link, axis=1)\n",
    "\n",
    "#     summarized_data_with_geoms = summarized_data_with_geoms.merge(\n",
    "#         temp_geoms[['data_origin','tmc_code','geometry_simplified']], \n",
    "#         how='left', on=['data_origin','tmc_code'])\n",
    "    \n",
    "    \n",
    "    # Filter that indicates which observations/rows did not have an \n",
    "    # associated geometry in the NPMRDS geodata.\n",
    "    null_geoms_filter = summarized_data_with_geoms['geometry'].isnull()\n",
    "    \n",
    "    # Populating the `geom_final` column with either the geometry found in the NPMRDS\n",
    "    # geodata (preferred), or with the simplified geometry created above (fallback).\n",
    "    summarized_data_with_geoms['geom_final'] = summarized_data_with_geoms['geometry']\n",
    "    summarized_data_with_geoms.loc[null_geoms_filter,'geom_final'] = (\n",
    "        summarized_data_with_geoms.loc[null_geoms_filter,'geometry_simplified'])\n",
    "    \n",
    "    # Populating the `geom_final_type` column with a flag that shows what type of \n",
    "    # geometry is in the `geom_final` column. \n",
    "    # If `geom_final_type`== 'original_tmc_shape', then the geometry in \n",
    "    # `geom_final` represents the original NPMRDS geometry. \n",
    "    # If `geom_final_type`== 'simplified_tmc_shape', then the geometry in \n",
    "    # `geom_final` represents the simplified geometry we created in this script\n",
    "    # just a few lines above. \n",
    "    summarized_data_with_geoms['geom_final_type'] = 'original_tmc_shape'\n",
    "    summarized_data_with_geoms.loc[null_geoms_filter,\n",
    "                                   'geom_final_type'] = 'simplified_tmc_shape'\n",
    "    \n",
    "    summarized_data_with_geoms = gpd.GeoDataFrame(\n",
    "        summarized_data_with_geoms.drop(['geometry','geometry_simplified']\n",
    "                                        ,axis=1),\n",
    "        crs='epsg:4326',geometry='geom_final')\n",
    "    \n",
    "    # Extracting the WKT data. Useful for exporting to CSV. \n",
    "    summarized_data_with_geoms['geom_wkt'] = gpd.array.to_wkt(\n",
    "        summarized_data_with_geoms.geom_final.values)\n",
    "    \n",
    "    # Exporting final data to disk\n",
    "    #summarized_data_with_geoms.to_file(main_data_geoms_filename,driver='GPKG',layer='main')    \n",
    "\n",
    "    return summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5a05f-c7e3-4f18-a96e-0e0c9829462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_standard_fhwa_timeslots(main_data):\n",
    "    '''\n",
    "    Adds timeslot and date window columns to `main_data`. These are the \n",
    "    standard timeslots used for the FHWA reliability computations:\n",
    "        -AM Peak:   Between 06am and 10am\n",
    "        -Mid-day:   Between 10am and 04pm\n",
    "        -PM Peak:   Between 04pm and 08pm\n",
    "        -Overnight: Between 08pm and 06am\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input DataFrame containing raw speed data for all links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    main_data : pd.DataFrame\n",
    "        DataFrame containing raw speed data for all links. \n",
    "        After this function is executed, the following columns get added:\n",
    "            -\"time_slot\"\n",
    "            -\"date_window\"\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Creating the thresholds for the definitions of peak and off-peak time slots\n",
    "    # and finding the observations that fall in each category/time slot.\n",
    "    # Time slots used are:\n",
    "    #    -AM Peak:   Between 06am and 10am\n",
    "    #    -Mid-day:   Between 10am and 04pm\n",
    "    #    -PM Peak:   Between 04pm and 08pm\n",
    "    #    -Overnight: Between 08pm and 06am\n",
    "    \n",
    "    am_peak = time_slot(time_start = pd.to_datetime('06:00 AM').time(),\n",
    "                        time_end = pd.to_datetime('10:00 AM').time(),\n",
    "                        include_start = True, \n",
    "                        include_end = False,\n",
    "                        inside_outside = 'inside',\n",
    "                        slot_name = 'am_peak')\n",
    "    \n",
    "    afternoon_offpeak = time_slot(time_start = pd.to_datetime('10:00 AM').time(),\n",
    "                                  time_end = pd.to_datetime('04:00 PM').time(),\n",
    "                                  include_start = True, \n",
    "                                  include_end = False,\n",
    "                                  inside_outside = 'inside',\n",
    "                                  slot_name = 'midday')\n",
    "    \n",
    "    pm_peak = time_slot(time_start = pd.to_datetime('04:00 PM').time(),\n",
    "                        time_end = pd.to_datetime('08:00 PM').time(),\n",
    "                        include_start = True, \n",
    "                        include_end = False,\n",
    "                        inside_outside = 'inside',\n",
    "                        slot_name = 'pm_peak')\n",
    "    \n",
    "    night = time_slot(time_start = pd.to_datetime('06:00 AM').time(),\n",
    "                      time_end = pd.to_datetime('08:00 PM').time(),\n",
    "                      include_start = False, \n",
    "                      include_end = True,\n",
    "                      inside_outside = 'outside',\n",
    "                      slot_name = 'overnight')\n",
    "    \n",
    "    all_time_slots = [am_peak, afternoon_offpeak, pm_peak, night]\n",
    "    \n",
    "    # Adding the peak/offpeak/etc category data back into the `main_data` DataFrame\n",
    "    for this_time_slot in all_time_slots:\n",
    "        main_data = this_time_slot.add_time_slot_data_to_main_data(main_data)\n",
    "    \n",
    "    # Creating the thresholds for the definitions of day-of-year windows and finding\n",
    "    # the observations that fall in each category/window.\n",
    "    # The windows used are:\n",
    "    #    -All days: Between Jan 1, 2019 and Dec 31, 2019\n",
    "    # Note: Currently, there is only one category that spans the entire year. \n",
    "    #       The functionality was built in for future projects, when we might want \n",
    "    #       to compare, say, speeds during the four seasons. \n",
    "    \n",
    "    year_val = str(main_data.measurement_tstamp.dt.year.unique()[0])\n",
    "    beg_yr = 'Jan 1, {}'.format(year_val)\n",
    "    end_yr = 'Dec 31, {}'.format(year_val)\n",
    "    \n",
    "    all_days_window = day_of_year_window(start_date=pd.to_datetime(beg_yr).day_of_year, \n",
    "                                         end_date=pd.to_datetime(end_yr).day_of_year, \n",
    "                                         include_start = True, \n",
    "                                         include_end = True,\n",
    "                                         inside_outside = 'inside',\n",
    "                                         window_name = 'all_days')\n",
    "    \n",
    "    main_data = all_days_window.add_window_data_to_main_data(main_data)\n",
    "    \n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3631bf-a191-493a-aae3-3217af217a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_group_summarize_fhwa(main_data):\n",
    "    '''\n",
    "    Defines the standard periods and summaries needed for calculating the \n",
    "    FHWA reliability values.\n",
    "    To see the formal definitions of these periods, see CFR 23 490.511 and \n",
    "    CFR 23 490.611:\n",
    "        https://www.ecfr.gov/current/title-23/chapter-I/subchapter-E/part-490/subpart-E/section-490.511\n",
    "        https://www.ecfr.gov/current/title-23/chapter-I/subchapter-E/part-490/subpart-F/section-490.611\n",
    "        https://www.law.cornell.edu/cfr/text/23/490.511\n",
    "        https://www.law.cornell.edu/cfr/text/23/490.611\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "        It is expected that this dataframe will contain the following columns:\n",
    "            -\"time_slot\"\n",
    "            -\"date_window\"\n",
    "            -\"day_of_week\"\n",
    "            -\"time\"\n",
    "            -\"day_of_year\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        DataFrame that contains all the standard summary data required for \n",
    "        FHWA's reliability calculations.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #--------------------------------------------------------\n",
    "    # Step 1: Summarizing data for AM Peaks (only weekdays) -\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'am_peak'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['am_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_ampeak = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # Step 2: Summarizing data for Mid-day (only weekdays) -\n",
    "    #-------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'midday'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['midday'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_midday = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------\n",
    "    # Step 3: Summarizing data for PM Peaks (only weekdays) -\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'pm_peak'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['pm_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_pmpeak = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------\n",
    "    # Step 4: Summarizing data for overnight - all days -\n",
    "    #----------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'overnight'\n",
    "    \n",
    "    # Filters for Weekends - only considering 6am to 8pm.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['overnight'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = np.repeat(True, len(main_data))\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                        day_of_year_filter & \n",
    "                        time_filter & \n",
    "                        day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_overnight = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------\n",
    "    # Step 5: Summarizing data for weekends - 6am to 8pm -\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'weekends'\n",
    "    \n",
    "    # Filters for Weekends - only considering 6am to 8pm.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['am_peak','midday','pm_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([5,6])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_weekends = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    # Adding extra detail about this summary's time slot\n",
    "    #summarized_data_weekends['time_slot'] = '6am_to_8pm'\n",
    "    \n",
    "    \n",
    "    #--------------------------------------\n",
    "    # Step 6: Summarizing data - All-time -\n",
    "    #--------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'alltime'\n",
    "    \n",
    "    # Filters for alltime averages\n",
    "    time_slot_filter   = np.repeat(True, len(main_data))\n",
    "    day_of_year        = np.repeat(True, len(main_data))\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = np.repeat(True, len(main_data))\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year &\n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    summarized_data_alltime = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    # Adding extra detail about this summary's time slot\n",
    "    #summarized_data_alltime['time_slot'] = 'all_hours'\n",
    "    \n",
    "    # Creating list with all the summaries from the previous step\n",
    "    all_summaries = [summarized_data_ampeak.reset_index(),\n",
    "                     summarized_data_midday.reset_index(),\n",
    "                     summarized_data_pmpeak.reset_index(),\n",
    "                     summarized_data_overnight.reset_index(),\n",
    "                     summarized_data_weekends.reset_index(),\n",
    "                     summarized_data_alltime.reset_index()]\n",
    "    \n",
    "    # Concatenating all of the summaries into one large DataFrame\n",
    "    all_summaries_concat = pd.concat(all_summaries, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return all_summaries_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6401e0-6615-428d-9b48-cc57316f3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliabily_mixed_traffic(all_summaries_concat, \n",
    "                                               main_tmc_data):\n",
    "    '''\n",
    "    Calculates travel time reliability for mixed traffic according to FHWA's \n",
    "    standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # For the mixed traffic data (i.e., for observations where \n",
    "    # data_origin is in ['inrix', 'npmrds_from_inrix_trucks_and_passveh']):\n",
    "    #    Calculate 80th percentile divided by 50th percentile for four summary \n",
    "    #    groups: am_peak, midday, pm_peak, weekends.\n",
    "    #    Then, take the highest one of all four. If that value is larger than \n",
    "    #    1.5, the segment is deemd \"unreliable\". Otherwise, the segment is \n",
    "    #    deemed \"reliable\".\n",
    "    \n",
    "\n",
    "    # Keeping only the relevant summaries\n",
    "    mixed_traffic_data = all_summaries_concat.loc[\n",
    "        (all_summaries_concat['summary_type'].isin(['am_peak', \n",
    "                                                    'midday', \n",
    "                                                    'pm_peak',\n",
    "                                                    'weekends'])) \n",
    "        & (all_summaries_concat['data_origin'].isin(['inrix',\n",
    "                                                     'npmrds_from_inrix_trucks_and_passveh']))\n",
    "        ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 80th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    mixed_traffic_data['LOTTR_80p_by_50p'] = (\n",
    "        mixed_traffic_data['ttime_80p'] / \n",
    "        mixed_traffic_data['ttime_50p'])\n",
    "    \n",
    "    # Calculating the maximum of the Travel Time Reliability across all \n",
    "    # summaries\n",
    "    reliability_summaries_mixed_traffic = (mixed_traffic_data\n",
    "        .groupby(['tmc_code','data_origin'])\n",
    "        .agg(\n",
    "            RawDataRows = pd.NamedAgg(column='count_obs', \n",
    "                                       aggfunc='sum'),\n",
    "            SummaryCount = pd.NamedAgg(column='LOTTR_80p_by_50p', \n",
    "                                       aggfunc='count'),\n",
    "            Reliability = pd.NamedAgg(column='LOTTR_80p_by_50p', \n",
    "                                                   aggfunc='max'))\n",
    "        .reset_index())\n",
    "    \n",
    "    reliability_summaries_mixed_traffic['Reliability_Type'] = 'Mixed_Traffic'\n",
    "    \n",
    "    # Dropping rows that didn't have summaries for all periods needed\n",
    "    reliability_summaries_mixed_traffic = (\n",
    "        reliability_summaries_mixed_traffic.loc[\n",
    "            reliability_summaries_mixed_traffic['SummaryCount'] == 4]\n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    reliability_summaries_mixed_traffic['Reliable'] = (\n",
    "        reliability_summaries_mixed_traffic['Reliability'] < 1.5)\n",
    "    \n",
    "    reliability_summaries_mixed_traffic = (reliability_summaries_mixed_traffic\n",
    "                                           [['tmc_code', 'data_origin', \n",
    "                                             'Reliability_Type', 'RawDataRows', \n",
    "                                             'SummaryCount', 'Reliability',\n",
    "                                             'Reliable']])\n",
    "    \n",
    "    return reliability_summaries_mixed_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac21520-e328-455b-8748-fae9ebf5e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliability_trucks(all_summaries_concat, main_tmc_data):\n",
    "    '''\n",
    "    Calculates truck travel time reliability according to FHWA's standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # For truck traffic (data_origin='npmrds_from_inrix_trucks'):\n",
    "    #    Calculate 95th percentile divided by 50th percentile for the five \n",
    "    #    summary groups: am_peak, midday, pm_peak, overnight and weekends.\n",
    "    #    Then, just take the maximum of those five values. \n",
    "    #\n",
    "    # Note: Please note that, for trucks, we also have to analyze the \n",
    "    #       \"overnight\" summary, which is absent in the mixed traffic \n",
    "    #       summaries.\n",
    "    \n",
    "    # Keeping only the relevant summaries\n",
    "    truck_data = all_summaries_concat.loc[\n",
    "        (all_summaries_concat['summary_type'].isin(['am_peak', \n",
    "                                                    'midday', \n",
    "                                                    'pm_peak',\n",
    "                                                    'overnight',\n",
    "                                                    'weekends'])) & \n",
    "        (all_summaries_concat['data_origin']=='npmrds_from_inrix_trucks')\n",
    "        ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 95th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    truck_data['LOTTR_95p_by_50p'] = (\n",
    "        truck_data['ttime_95p'] / \n",
    "        truck_data['ttime_50p'])\n",
    "    \n",
    "    # Calculating the maximum of the Travel Time Reliability across all \n",
    "    # summaries\n",
    "    reliability_summaries_truck_traffic = (truck_data\n",
    "        .groupby(['tmc_code','data_origin'])\n",
    "        .agg(\n",
    "            RawDataRows = pd.NamedAgg(column='count_obs', \n",
    "                                       aggfunc='sum'),\n",
    "            SummaryCount = pd.NamedAgg(column='LOTTR_95p_by_50p', \n",
    "                                       aggfunc='count'),\n",
    "            Reliability = pd.NamedAgg(column='LOTTR_95p_by_50p', \n",
    "                                                   aggfunc='max'))\n",
    "        .reset_index())\n",
    "\n",
    "    reliability_summaries_truck_traffic['Reliability_Type'] = 'Truck_Traffic'\n",
    "\n",
    "    # Dropping rows that didn't have summaries for all periods needed\n",
    "    reliability_summaries_truck_traffic = (\n",
    "        reliability_summaries_truck_traffic.loc[\n",
    "            reliability_summaries_truck_traffic['SummaryCount'] == 5]\n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    reliability_summaries_truck_traffic['Reliable'] = (\n",
    "        reliability_summaries_truck_traffic['Reliability'] < 1.5)\n",
    "    \n",
    "    reliability_summaries_truck_traffic = (reliability_summaries_truck_traffic\n",
    "                                           [['tmc_code', 'data_origin', \n",
    "                                             'Reliability_Type', 'RawDataRows', \n",
    "                                             'SummaryCount', 'Reliability',\n",
    "                                             'Reliable']])\n",
    "    \n",
    "    return reliability_summaries_truck_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed24ee-fed2-4ea8-af74-6ec5488199de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliabilities(all_summaries_concat, \n",
    "                                     main_data, \n",
    "                                     main_tmc_data,\n",
    "                                     calc_mixed_traf_rel=True,\n",
    "                                     calc_truck_rel=True):\n",
    "    '''\n",
    "    Calculates the overall Reliability according to FHWA's standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "    calc_mixed_traf_rel : boolean\n",
    "        Flag that signals whether or not to calculate reliability for mixed \n",
    "        traffic\n",
    "    calc_truck_rel : boolean\n",
    "        Flag that signals whether or not to calculate reliability for trucks\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # Calculating overall Reliability. \n",
    "    \n",
    "    # For the mixed traffic data (data_origin='npmrds_from_inrix_trucks_and_passveh'):\n",
    "    #    Calculate 80th percentile divided by 50th percentile for four summary \n",
    "    #    groups: am_peak, midday, pm_peak, weekends.\n",
    "    #    Then, take the highest one of all four. If that value is larger than 1.5, \n",
    "    #    the segment is deemed \"unreliable\". Otherwise, the segment is deemed \"reliable\".\n",
    "    \n",
    "    # For truck traffic (data_origin='npmrds_from_inrix_trucks'):\n",
    "    #    Calculate 95th percentile divided by 50th percentile for the five summary\n",
    "    #    groups: am_peak, midday, pm_peak, overnight and weekends.\n",
    "    #    Then, just take the maximum of those five values. \n",
    "    #    Note: For trucks, this is a continuous value, while for mixed traffic the \n",
    "    #    metric is just a binary \"reliable\"/\"unreliable\" variable.\n",
    "    \n",
    "    # List that will store all reliability results\n",
    "    reliability_dfs = []\n",
    "    \n",
    "    # Calculating reliability for mixed traffic\n",
    "    if calc_mixed_traf_rel:\n",
    "        reliability_summaries_mixed_traffic = (\n",
    "            calculate_standard_reliabily_mixed_traffic(all_summaries_concat, \n",
    "                                                       main_tmc_data))\n",
    "        \n",
    "        # Finding missing TMC codes and re-including them\n",
    "        missing_tmc_codes_mixed_traf = find_missing_tmc_codes(main_data, \n",
    "                                                   reliability_summaries_mixed_traffic)\n",
    "        \n",
    "        missing_tmc_codes_mixed_traf_df = (\n",
    "            pd.DataFrame({'tmc_code':missing_tmc_codes_mixed_traf,\n",
    "                          'data_origin':reliability_summaries_mixed_traffic['data_origin'].values[0]}))\n",
    "        \n",
    "        reliability_summaries_mixed_traffic = (\n",
    "            pd.concat([reliability_summaries_mixed_traffic, \n",
    "                       missing_tmc_codes_mixed_traf_df],\n",
    "                      ignore_index=True).reset_index(drop=True))\n",
    "\n",
    "        reliability_dfs.append(reliability_summaries_mixed_traffic)\n",
    "    \n",
    "    # Calculating reliability for trucks\n",
    "    if calc_truck_rel:\n",
    "        reliability_summaries_truck_traffic = (\n",
    "            calculate_standard_reliability_trucks(all_summaries_concat, \n",
    "                                                  main_tmc_data))\n",
    "        \n",
    "        missing_tmc_codes_truck = find_missing_tmc_codes(main_data, \n",
    "                                                   reliability_summaries_truck_traffic)\n",
    "        # Finding missing TMC codes and re-including them\n",
    "        missing_tmc_codestruck_df = (\n",
    "                    pd.DataFrame({'tmc_code':missing_tmc_codes_truck,\n",
    "                                  'data_origin':reliability_summaries_truck_traffic['data_origin'].values[0]}))\n",
    "                \n",
    "        reliability_summaries_truck_traffic = (\n",
    "            pd.concat([reliability_summaries_truck_traffic, \n",
    "                       missing_tmc_codestruck_df],\n",
    "                      ignore_index=True).reset_index(drop=True))\n",
    "        \n",
    "        reliability_dfs.append(reliability_summaries_truck_traffic)\n",
    "    \n",
    "    # Combining mixed traffic and truck reliability data\n",
    "    reliability_summaries_all = pd.concat(reliability_dfs,\n",
    "                                          ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return reliability_summaries_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01d887-26d2-48b8-aa6e-74b117df1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_tmc_codes(main_data, ref_data):\n",
    "    '''\n",
    "    Finds which TMC codes are missing in the `ref_data` from the original raw\n",
    "    dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "    ref_data : pd.DataFrame\n",
    "        Reference data whose TMC codes will be checked against the original\n",
    "        raw data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    missing_tmc_codes : LIST\n",
    "        List of TMC codes that are \"missing\" from `ref_data` (i.e., they exist\n",
    "        in `main_data`, but not in `ref_data`).\n",
    "\n",
    "    '''\n",
    "    # Getting unique TMC codes from both sets\n",
    "    main_data_unique = main_data['tmc_code'].unique()\n",
    "    ref_data_unique  = set(ref_data['tmc_code'].unique())\n",
    "    \n",
    "    # List that will hold missing TMC codes\n",
    "    missing_tmc_codes = []\n",
    "    \n",
    "    # Comparing the sets and finding which ones are missing\n",
    "    trash = pd.Series(main_data_unique).apply(\n",
    "        lambda x: missing_tmc_codes.append(x) \n",
    "            if x not in ref_data_unique \n",
    "            else None)\n",
    "    \n",
    "    return missing_tmc_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392e293-935c-4853-9f51-54452a492572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_to_finish_fhwa_summaries_and_reliability(\n",
    "        input_data_folder='',\n",
    "        npmrds_geodata_path='',\n",
    "        road_str='',\n",
    "        chunk_size=100000,\n",
    "        export_raw_speed_data=False,\n",
    "        output_raw_data_filename_pickle='',\n",
    "        export_tmc_data=False,\n",
    "        output_tmc_data_filename_pickle='',\n",
    "        export_summary_data=False,\n",
    "        output_summary_data_filename_gpkg='',\n",
    "        export_reliability_data=False,\n",
    "        output_reliability_data_filename_gpkg=''):\n",
    "    '''\n",
    "    Does everything needed to calculate the FHWA reliability metrics from the \n",
    "    zipped raw data files from RITIS. \n",
    "    The several bells and whistles in this function's inputs are just controls\n",
    "    of whether or not to export some of the processed datasets to the local disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "    npmrds_geodata_path : STR\n",
    "        String that identifies where to find the NPMRDS shapefile. Needs to \n",
    "        point to the \".shp\" file. Typically, this file is just called \"Texas.shp\"\n",
    "    road_str: STR \n",
    "        String used to filter road segments based on their names. This is also \n",
    "        referred to as \"the main searched road\" in other places of this script.\n",
    "        The TMC segments will be filtered based on whether or not the 'road' \n",
    "        column contains this string. To get the entire dataset back, just use \n",
    "        an empty string ('').\n",
    "    chunk_size: INT\n",
    "        Integer used to determine number of rows read at a time by Pandas' \n",
    "        read_csv method.\n",
    "    export_raw_speed_data : bool\n",
    "        Determines whether or not to export the raw speed data to disk. \n",
    "        The default is False.\n",
    "    output_raw_data_filename_pickle : STR\n",
    "        Full (absolute) path of the PICKLE file containing the raw data\n",
    "        read in through this function.\n",
    "    export_tmc_data : bool\n",
    "        Determines whether or not to export the TMC Information data to disk. \n",
    "        The default is False.\n",
    "    output_tmc_data_filename_pickle : STR\n",
    "        Full (absolute) path of the PICKLE file containing the TMC data\n",
    "        read in through this function.\n",
    "    export_summary_data : bool\n",
    "        Determines whether or not to export the summary data to disk. \n",
    "        The default is False.\n",
    "    output_summary_data_filename_gpkg : STR\n",
    "        String that identifies the path and filename to give to the GeoDataFrame\n",
    "        that contains the summary data. This needs to be a GeoPackage\n",
    "        file ('.gpkg' extension).\n",
    "    export_reliability_data : bool\n",
    "        Determines whether or not to export the reliability data to disk. \n",
    "        The default is False.\n",
    "    output_reliability_data_filename_gpkg : STR\n",
    "        String that identifies the path and filename to give to the GeoDataFrame\n",
    "        that contains the reliability data. This needs to be a GeoPackage\n",
    "        file ('.gpkg' extension). The default is ''.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_dict : DICT\n",
    "        Dictionary containing four datasets:\n",
    "            -main_data: pd.DataFrame that contains all the raw data with the \n",
    "                extra processing columns \n",
    "            -main_tmc_data: pd.DataFrame that contains the TMC information\n",
    "                about all the links\n",
    "            -all_summaries_with_geoms: gpd.GeoDataFrame that contains all the\n",
    "                summary data\n",
    "            -reliability_summaries_with_geoms: gpd.GeoDataFrame that contains\n",
    "                all the reliability data\n",
    "    '''\n",
    "    \n",
    "    ############################\n",
    "    ### STEP 1: READING DATA ###\n",
    "    ############################\n",
    "    \n",
    "    # Actually reading in the whole data and generating the filtered output files\n",
    "    all_data = read_batch_of_raw_data(input_data_folder, \n",
    "                                      road_str, \n",
    "                                      chunk_size)\n",
    "    \n",
    "    # Fishing out the `main_data` and `main_tmc_data` DataFrames.\n",
    "    main_data = all_data['main_data']\n",
    "    main_tmc_data  = all_data['main_tmc_data']\n",
    "\n",
    "    # Exporting raw data and TMC information\n",
    "    if export_raw_speed_data:\n",
    "        main_data.to_pickle(output_raw_data_filename_pickle)\n",
    "    \n",
    "    if export_tmc_data:\n",
    "        main_tmc_data.to_pickle(output_tmc_data_filename_pickle)\n",
    "\n",
    "    ###################################################\n",
    "    ### STEP 2: PRE-POCESSING COLUMNS FOR FILTERING ###\n",
    "    ###################################################\n",
    "    \n",
    "    # Fixing datetime information: adding time and day_of_week columns\n",
    "    main_data = fix_datetime_columns(main_data)\n",
    "    \n",
    "    # Adding timeslot and date window columns\n",
    "    main_data = define_standard_fhwa_timeslots(main_data)\n",
    "    \n",
    "    ############################################################\n",
    "    ### STEP 3: FILTERING, GROUPING AND SUMMARIZING THE DATA ###\n",
    "    ############################################################\n",
    "    \n",
    "    all_summaries_concat = filter_group_summarize_fhwa(main_data)\n",
    "    \n",
    "    all_summaries_with_geoms = add_geometries_to_summaries(\n",
    "                                   summarized_data=all_summaries_concat, \n",
    "                                   main_tmc_data=main_tmc_data,\n",
    "                                   npmrds_geodata_path=npmrds_geodata_path)\n",
    "    \n",
    "    if export_summary_data:\n",
    "        all_summaries_with_geoms.to_file(output_summary_data_filename_gpkg, \n",
    "                                         driver='GPKG',\n",
    "                                         layer='main')\n",
    "    \n",
    "    ################################################\n",
    "    ### STEP 4: CALCULATING RELIABILITY MEASURES ###\n",
    "    ################################################\n",
    "    \n",
    "    reliability_summaries_all = calculate_standard_reliabilities(\n",
    "        all_summaries_concat=all_summaries_concat, \n",
    "        main_data=main_data, \n",
    "        main_tmc_data=main_tmc_data,\n",
    "        calc_mixed_traf_rel=True,\n",
    "        calc_truck_rel=True)\n",
    "    \n",
    "    reliability_summaries_with_geoms = add_geometries_to_summaries(\n",
    "        summarized_data=reliability_summaries_all, \n",
    "        main_tmc_data=main_tmc_data,\n",
    "        npmrds_geodata_path=npmrds_geodata_path)\n",
    "    \n",
    "    if export_reliability_data:\n",
    "        reliability_summaries_with_geoms.to_file(output_reliability_data_filename_gpkg, \n",
    "                                                 driver='GPKG',\n",
    "                                                 layer='main')\n",
    "    \n",
    "    output_dict = {'main_data':main_data,\n",
    "                   'main_tmc_data':main_tmc_data,\n",
    "                   'all_summaries_with_geoms':all_summaries_with_geoms,\n",
    "                   'reliability_summaries_with_geoms':reliability_summaries_with_geoms}\n",
    "    \n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4812d-7caf-40c4-ab95-5d5e44918aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17aa822-c248-4d9e-ac31-3ce118a4f247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed753e-9a3f-4915-b9ca-07f3e95967f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e261f5-3e99-44d1-98a8-4e0f32f59df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_data_folder = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022'\n",
    "input_data = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022\\Hawaii_YR2022_15-mins.zip'\n",
    "# data_paths_dict = which_zip_is_which_data_source(input_data_folder)\n",
    "\n",
    "# data_origin = 'npmrds_from_inrix_trucks'\n",
    "# raw_data_zipfile = data_paths_dict['npmrds_from_inrix_trucks']['zip_file']\n",
    "# raw_data_filename_in_zip = data_paths_dict['npmrds_from_inrix_trucks']['raw_data_file']\n",
    "road_str = ''\n",
    "chunk_size = 100000\n",
    "\n",
    "# output_dict = read_csv_get_specific_road_segments(data_origin,\n",
    "#                                         raw_data_zipfile,\n",
    "#                                         raw_data_filename_in_zip,\n",
    "#                                         road_str,chunk_size,\n",
    "#                                         raw_data_chunks=None,\n",
    "#                                         tmc_data_parts=None)\n",
    "\n",
    "# raw_data_chunks = output_dict['raw_data_chunks']\n",
    "# tmc_data_parts = output_dict['tmc_data_parts']\n",
    "\n",
    "test = read_batch_of_raw_data(input_data_folder,road_str,chunk_size)\n",
    "main_data = test['main_data']\n",
    "main_tmc_data = test['main_tmc_data']\n",
    "\n",
    "main_data = fix_datetime_columns(main_data)\n",
    "main_data = define_standard_fhwa_timeslots(main_data)\n",
    "all_summaries_concat = filter_group_summarize_fhwa(main_data)\n",
    "\n",
    "calculate_standard_reliability_trucks(all_summaries_concat, main_tmc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210b77d-c6c5-4382-85be-eca74242c809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5a774d-702a-4732-b6e9-1099fa69e043",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Modified Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad57adf-70bc-4e7b-856f-cd85440bac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_zip_is_which_data_source(input_data_folder):\n",
    "    '''\n",
    "    Function that searches the input data folder for zip files and determines \n",
    "    which zipfiles contain the data needed for this task. The function also \n",
    "    identifies the type of data/data source for each of those zipfiles. \n",
    "    For example: 'texas_inrix_npmrds_15min(1).zip' contains the 'NPMRDS from \n",
    "    INRIX (Passenger vehicles)' data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_paths_dict: DICT\n",
    "        Dictionary that contains information about where the files for each \n",
    "        data source is located. It should be structured as follows:\n",
    "            {'data_origin_1':{'zip_file':'zip_file_1_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_1.csv'},\n",
    "             'data_origin_2':{'zip_file':'zip_file_2_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_2.csv'},\n",
    "             ...}\n",
    "    '''\n",
    "    \n",
    "    # Dictionary that is used to match data origin to the RegEx string\n",
    "    dict_for_origin_match = {\n",
    "        'inrix':\n",
    "            '.*INRIX TMC.*',\n",
    "        'npmrds_from_inrix_pass_vehicles':\n",
    "            '.*NPMRDS from INRIX \\(Passenger vehicles\\).*',\n",
    "        'npmrds_from_inrix_trucks':\n",
    "            '.*NPMRDS from INRIX \\(Trucks\\).*',\n",
    "        'npmrds_from_inrix_trucks_and_passveh':\n",
    "            '.*NPMRDS from INRIX \\(Trucks and passenger vehicles\\).*'}\n",
    "    \n",
    "    # Dictionary that will store the output\n",
    "    data_paths_dict = {}\n",
    "\n",
    "    with zipfile.ZipFile(input_data_folder) as this_zip:\n",
    "            \n",
    "        # Checking if this is a data extract from RITIS' massive data downloader\n",
    "        files_in_zip = this_zip.namelist()\n",
    "        if 'Contents.txt' in files_in_zip:\n",
    "                \n",
    "            #Extracting the name of the raw data CSV file inside this zipfile\n",
    "            raw_data_file = [this_file for this_file in files_in_zip if \n",
    "                                 this_file !='Contents.txt' and \n",
    "                                 this_file !='TMC_Identification.csv'][0]\n",
    "                \n",
    "            # Performing a RegEx search to find which data source this \n",
    "            # zipfile originally came from\n",
    "            with this_zip.open('Contents.txt','r') as content_file:\n",
    "                    this_content = content_file.readline().decode('utf-8')\n",
    "                    for this_data_origin, this_regex_string in (\n",
    "                            dict_for_origin_match.items()):\n",
    "                        regex_search = re.match(this_regex_string,this_content)\n",
    "                        if regex_search:\n",
    "                            data_paths_dict[this_data_origin] = {\n",
    "                                'zip_file':str(input_data_folder),\n",
    "                                'raw_data_file':raw_data_file}\n",
    "    return data_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157f867-5659-47b0-ad1b-b87dde47a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliability(all_summaries_concat, \n",
    "                                               main_tmc_data):\n",
    "    '''\n",
    "    Calculates travel time reliability for mixed traffic according to FHWA's \n",
    "    standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    reliability_dfs = []\n",
    "    \n",
    "    # For the mixed traffic data (i.e., for observations where \n",
    "    # data_origin is in ['inrix', 'npmrds_from_inrix_trucks_and_passveh']):\n",
    "    #    Calculate 80th percentile divided by 50th percentile for four summary \n",
    "    #    groups: am_peak, midday, pm_peak, weekends.\n",
    "    #    Then, take the highest one of all four. If that value is larger than \n",
    "    #    1.5, the segment is deemd \"unreliable\". Otherwise, the segment is \n",
    "    #    deemed \"reliable\".\n",
    "    \n",
    "\n",
    "    # Keeping only the relevant summaries\n",
    "    mixed_traffic_data = all_summaries_concat.loc[(all_summaries_concat['data_origin']=='npmrds_from_inrix_trucks_and_passveh')\n",
    "                                                 ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 80th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    mixed_traffic_data['Reliability'] = (\n",
    "        mixed_traffic_data['ttime_80p'] / \n",
    "        mixed_traffic_data['ttime_50p'])\n",
    "    \n",
    "    mixed_traffic_data['Reliability_Type'] = 'Mixed_Traffic_80p_50p'\n",
    "    \n",
    "    \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    mixed_traffic_data['Reliable'] = (\n",
    "        mixed_traffic_data['Reliability'] < 1.5)\n",
    "    \n",
    "    mixed_traffic_data = (mixed_traffic_data[['tmc_code', 'data_origin','summary_type','speed_avg','ttime_avg', \n",
    "                                              'Reliability_Type','Reliability','Reliable']])\n",
    "    \n",
    "    reliability_dfs.append(mixed_traffic_data)\n",
    "    \n",
    "    \n",
    "    # Keeping only the relevant summaries\n",
    "    truck_data = all_summaries_concat.loc[(all_summaries_concat['data_origin']=='npmrds_from_inrix_trucks')\n",
    "                                         ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 95th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    truck_data['Reliability'] = (\n",
    "        truck_data['ttime_95p'] / \n",
    "        truck_data['ttime_50p'])\n",
    "    \n",
    "    truck_data['Reliability_Type'] = 'Truck_Traffic_95p_50p'\n",
    "\n",
    "   \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    truck_data['Reliable'] = (\n",
    "        truck_data['Reliability'] < 1.5)\n",
    "    \n",
    "    truck_data = (truck_data[['tmc_code', 'data_origin','summary_type','speed_avg','ttime_avg', \n",
    "                              'Reliability_Type','Reliability','Reliable']])\n",
    "    \n",
    "    reliability_dfs.append(truck_data)\n",
    "    \n",
    "    # Combining mixed traffic and truck reliability data\n",
    "    reliability_summaries_all = pd.concat(reliability_dfs,\n",
    "                                          ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return reliability_summaries_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a139f1-e859-4b48-a5a3-62d5557d3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometries_to_summaries(summarized_data,\n",
    "                                npmrds_geodata_path):\n",
    "    '''\n",
    "    Adds a column called \"geom_final\" to the dataset. This new column contains \n",
    "    a geometry for each row in the summary dataset. This geometry is generated \n",
    "    in one of two different ways, in the following priority:\n",
    "        -Look in the NPMRDS shapefile to try and find a link with matching TMC\n",
    "        -If we can't find one, we just draw a straight line from the lat/lon\n",
    "            data that is found in the TMC_Identification.csv files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summarized_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the speed summaries. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the \n",
    "        TMC segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "    npmrds_geodata_path : STR\n",
    "        String that identifies where to find the NPMRDS shapefile. Needs to \n",
    "        point to the \".shp\" file. Typically, this file is just called \"Texas.shp\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    summarized_data_with_geoms : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing the geometries associated with each link.\n",
    "    \n",
    "    '''\n",
    "    def renaming_fun(x):\n",
    "        if x == \"Tmc\"or x == \"tmc\":\n",
    "            return \"tmc_code\"\n",
    "        return x\n",
    "\n",
    "    npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename(columns=renaming_fun)\n",
    "    # npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename({'Tmc':'tmc_code'},\n",
    "    #                                                            axis=1)\n",
    "    \n",
    "    # Merging summaries with NPMRDS geometries\n",
    "    summarized_data_with_geoms = summarized_data.merge(\n",
    "        npmrds_geodata[['tmc_code', 'roadname', 'faciltype', 'nhs', 'strhnt_typ', 'isprimary', 'geometry']].to_crs('epsg:4326'), \n",
    "        how='left', \n",
    "        on='tmc_code').dropna(subset=['geometry'])\n",
    "    \n",
    "    # summarized_data_with_geoms = summarized_data.merge(\n",
    "    #     main_tmc_data[['tmc_code', 'road', 'faciltype', 'nhs', 'strhnt_typ', 'isprimary']], \n",
    "    #     how='left', \n",
    "    #     on='tmc_code')\n",
    "    \n",
    "    \n",
    "    # # Extracting the WKT data. Useful for exporting to CSV. \n",
    "    # summarized_data_with_geoms['geom_wkt'] = gpd.array.to_wkt(\n",
    "    #     summarized_data_with_geoms.geometry.values)\n",
    "    \n",
    "    # Exporting final data to disk\n",
    "    #summarized_data_with_geoms.to_file(main_data_geoms_filename,driver='GPKG',layer='main')    \n",
    "\n",
    "    return summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ff0ad-1ece-4925-88bf-e150c94ca880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3a04f-f4a7-494b-ae83-3ad9a3414382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574c788-b87c-46ed-ad7e-4e0e26f8edb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06ecf7-b355-41fb-acc1-5dc96ee59417",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022\\Hawaii_YR2022_15-mins.zip'\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(input_data) as arch:\n",
    "        arch.printdir()\n",
    "except zipfile.BadZipFile as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0765e-1702-4577-8ed3-8aeea47266f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_data_folder = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022\\Hawaii_YR2022_15-mins.zip'\n",
    "npmrds_geodata_path = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\npmrds_links\\Hawaii\\Hawaii.shp'\n",
    "\n",
    "test = read_batch_of_raw_data(input_data_folder,road_str,chunk_size)\n",
    "main_data = test['main_data']\n",
    "main_tmc_data = test['main_tmc_data']\n",
    "\n",
    "main_data = fix_datetime_columns(main_data)\n",
    "main_data = define_standard_fhwa_timeslots(main_data)\n",
    "all_summaries_concat = filter_group_summarize_fhwa(main_data)\n",
    "\n",
    "reliability_summaries_all = calculate_standard_reliability(all_summaries_concat, main_tmc_data)\n",
    "summarized_data_with_geoms = add_geometries_to_summaries(reliability_summaries_all, npmrds_geodata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af908a4-5cea-41d9-9d45-82926b902e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc74d8f-2026-40c5-a02c-72dfebf91c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gpd.GeoDataFrame(\n",
    "    summarized_data_with_geoms, geometry=summarized_data_with_geoms.geometry, crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2633750-40bc-4492-88bf-993505074a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_file('Hawaii_YR2022.gpkg',driver='GPKG',layer='main') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6284952-c35a-403f-af8e-1fcbe9cd8972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ff05ba-bcc0-44ad-92c7-99cefeeefc21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d40292-48c1-4493-beec-ba466808009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "# # import streamlit as st\n",
    "# import folium\n",
    "\n",
    "sys.path.append('apps')\n",
    "import inrix_npmrds_functions as inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96f0f43-375a-44d6-993b-03307253b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inrix_pipeline(input_data_folder,road_str,chunk_size, npmrds_geodata_path):\n",
    "    test = inr.read_batch_of_raw_data(input_data_folder,road_str,chunk_size)\n",
    "    main_data = test['main_data']\n",
    "    main_tmc_data = test['main_tmc_data']\n",
    "\n",
    "    main_data = inr.fix_datetime_columns(main_data)\n",
    "    main_data = inr.define_standard_fhwa_timeslots(main_data)\n",
    "    all_summaries_concat = inr.filter_group_summarize_fhwa(main_data)\n",
    "\n",
    "    reliability_summaries_all = inr.calculate_standard_reliability(all_summaries_concat, main_tmc_data)\n",
    "    summarized_data_with_geoms = inr.add_geometries_to_summaries(reliability_summaries_all, npmrds_geodata_path)\n",
    "\n",
    "    return summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63f0b38-a16b-4396-b6a4-5fdb4a36c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 36s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>summary_type</th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability_Type</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>Reliable</th>\n",
       "      <th>roadname</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "      <th>isprimary</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126+04098</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>39.465676</td>\n",
       "      <td>6.766911</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>2.955993</td>\n",
       "      <td>False</td>\n",
       "      <td>HI-72 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.77996 21.27795, -157.78063 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126+04099</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>39.695578</td>\n",
       "      <td>39.613949</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.090194</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.78392 21.27838, -157.78417 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126+04100</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>44.484172</td>\n",
       "      <td>57.152976</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.099972</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.79424 21.27856, -157.79440 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126+04101</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>42.245109</td>\n",
       "      <td>70.743545</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.797780</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.80222 21.28137, -157.80244 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126+04102</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>39.585800</td>\n",
       "      <td>34.898510</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>4.661359</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.80985 21.28528, -157.81021 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12425</th>\n",
       "      <td>126P50017</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>24.272889</td>\n",
       "      <td>6.666648</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>False</td>\n",
       "      <td>HI-32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-156.49962 20.88826, -156.49941 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12426</th>\n",
       "      <td>126P50020</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>25.554786</td>\n",
       "      <td>4.049340</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.903804</td>\n",
       "      <td>False</td>\n",
       "      <td>HI-32 E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-156.47867 20.88876, -156.47828 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12427</th>\n",
       "      <td>126P50023</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>15.385396</td>\n",
       "      <td>19.275964</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>2.270935</td>\n",
       "      <td>False</td>\n",
       "      <td>HI-32 E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-156.46422 20.89254, -156.46375 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12428</th>\n",
       "      <td>126P50025</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>39.474119</td>\n",
       "      <td>2.878283</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.249097</td>\n",
       "      <td>True</td>\n",
       "      <td>AKONI PULE HWY</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-155.81291 20.02275, -155.81301 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>126P50036</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>14.738623</td>\n",
       "      <td>7.695289</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>2.499200</td>\n",
       "      <td>False</td>\n",
       "      <td>KAMEHAMEHA AVE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-155.06413 19.72257, -155.06398 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6595 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tmc_code               data_origin summary_type  speed_avg  ttime_avg  \\\n",
       "0      126+04098  npmrds_from_inrix_trucks      am_peak  39.465676   6.766911   \n",
       "1      126+04099  npmrds_from_inrix_trucks      am_peak  39.695578  39.613949   \n",
       "2      126+04100  npmrds_from_inrix_trucks      am_peak  44.484172  57.152976   \n",
       "3      126+04101  npmrds_from_inrix_trucks      am_peak  42.245109  70.743545   \n",
       "4      126+04102  npmrds_from_inrix_trucks      am_peak  39.585800  34.898510   \n",
       "...          ...                       ...          ...        ...        ...   \n",
       "12425  126P50017  npmrds_from_inrix_trucks      alltime  24.272889   6.666648   \n",
       "12426  126P50020  npmrds_from_inrix_trucks      alltime  25.554786   4.049340   \n",
       "12427  126P50023  npmrds_from_inrix_trucks      alltime  15.385396  19.275964   \n",
       "12428  126P50025  npmrds_from_inrix_trucks      alltime  39.474119   2.878283   \n",
       "12434  126P50036  npmrds_from_inrix_trucks      alltime  14.738623   7.695289   \n",
       "\n",
       "            Reliability_Type  Reliability  Reliable        roadname  \\\n",
       "0      Truck_Traffic_95p_50p     2.955993     False         HI-72 W   \n",
       "1      Truck_Traffic_95p_50p     5.090194     False          I-H1 W   \n",
       "2      Truck_Traffic_95p_50p     5.099972     False          I-H1 W   \n",
       "3      Truck_Traffic_95p_50p     5.797780     False          I-H1 W   \n",
       "4      Truck_Traffic_95p_50p     4.661359     False          I-H1 W   \n",
       "...                      ...          ...       ...             ...   \n",
       "12425  Truck_Traffic_95p_50p     1.846154     False           HI-32   \n",
       "12426  Truck_Traffic_95p_50p     1.903804     False         HI-32 E   \n",
       "12427  Truck_Traffic_95p_50p     2.270935     False         HI-32 E   \n",
       "12428  Truck_Traffic_95p_50p     1.249097      True  AKONI PULE HWY   \n",
       "12434  Truck_Traffic_95p_50p     2.499200     False  KAMEHAMEHA AVE   \n",
       "\n",
       "       faciltype  nhs  strhnt_typ isprimary  \\\n",
       "0            2.0  1.0         1.0         1   \n",
       "1            2.0  1.0         1.0         1   \n",
       "2            2.0  1.0         1.0         1   \n",
       "3            2.0  1.0         1.0         1   \n",
       "4            2.0  1.0         1.0         1   \n",
       "...          ...  ...         ...       ...   \n",
       "12425        2.0  1.0         NaN         1   \n",
       "12426        2.0  1.0         NaN         1   \n",
       "12427        2.0  1.0         NaN         1   \n",
       "12428        2.0  1.0         NaN         1   \n",
       "12434        2.0  1.0         NaN         1   \n",
       "\n",
       "                                                geometry  \n",
       "0      LINESTRING (-157.77996 21.27795, -157.78063 21...  \n",
       "1      LINESTRING (-157.78392 21.27838, -157.78417 21...  \n",
       "2      LINESTRING (-157.79424 21.27856, -157.79440 21...  \n",
       "3      LINESTRING (-157.80222 21.28137, -157.80244 21...  \n",
       "4      LINESTRING (-157.80985 21.28528, -157.81021 21...  \n",
       "...                                                  ...  \n",
       "12425  LINESTRING (-156.49962 20.88826, -156.49941 20...  \n",
       "12426  LINESTRING (-156.47867 20.88876, -156.47828 20...  \n",
       "12427  LINESTRING (-156.46422 20.89254, -156.46375 20...  \n",
       "12428  LINESTRING (-155.81291 20.02275, -155.81301 20...  \n",
       "12434  LINESTRING (-155.06413 19.72257, -155.06398 19...  \n",
       "\n",
       "[6595 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_data_folder = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022\\Hawaii_YR2022_15-mins.zip'\n",
    "npmrds_geodata_path = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\npmrds_links\\Hawaii\\Hawaii.shp'\n",
    "road_str = ''\n",
    "chunk_size = 100000\n",
    "\n",
    "summarized_data_with_geoms = process_inrix_pipeline(input_data_folder,road_str,chunk_size, npmrds_geodata_path)\n",
    "summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96ffe5a-99ef-495f-9ffa-f51e95150a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1101.000000</td>\n",
       "      <td>1101.000000</td>\n",
       "      <td>1101.000000</td>\n",
       "      <td>1101.000000</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.214102</td>\n",
       "      <td>98.683696</td>\n",
       "      <td>2.290420</td>\n",
       "      <td>1.959128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.297574</td>\n",
       "      <td>164.579873</td>\n",
       "      <td>0.923933</td>\n",
       "      <td>0.198083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.129630</td>\n",
       "      <td>0.736676</td>\n",
       "      <td>1.113101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.203348</td>\n",
       "      <td>11.490000</td>\n",
       "      <td>1.656145</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.068039</td>\n",
       "      <td>34.741572</td>\n",
       "      <td>2.166697</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.560194</td>\n",
       "      <td>117.247841</td>\n",
       "      <td>2.690967</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>60.367163</td>\n",
       "      <td>1283.813290</td>\n",
       "      <td>10.502078</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speed_avg    ttime_avg  Reliability    faciltype     nhs  strhnt_typ\n",
       "count  1101.000000  1101.000000  1101.000000  1101.000000  1101.0       345.0\n",
       "mean     32.214102    98.683696     2.290420     1.959128     1.0         1.0\n",
       "std      13.297574   164.579873     0.923933     0.198083     0.0         0.0\n",
       "min       7.129630     0.736676     1.113101     1.000000     1.0         1.0\n",
       "25%      21.203348    11.490000     1.656145     2.000000     1.0         1.0\n",
       "50%      31.068039    34.741572     2.166697     2.000000     1.0         1.0\n",
       "75%      42.560194   117.247841     2.690967     2.000000     1.0         1.0\n",
       "max      60.367163  1283.813290    10.502078     2.000000     1.0         1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = gpd.GeoDataFrame(summarized_data_with_geoms, geometry=summarized_data_with_geoms.geometry, crs=\"EPSG:4326\")\n",
    "final[final['summary_type'] == 'alltime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842880da-edeb-487f-bb37-e36e7e8a350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1054.000000</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.390528</td>\n",
       "      <td>84.592619</td>\n",
       "      <td>1.951307</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.314169</td>\n",
       "      <td>144.149564</td>\n",
       "      <td>0.867956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.773512</td>\n",
       "      <td>0.630255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.262763</td>\n",
       "      <td>9.191569</td>\n",
       "      <td>1.308689</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.228934</td>\n",
       "      <td>29.798765</td>\n",
       "      <td>1.650016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.586196</td>\n",
       "      <td>93.482095</td>\n",
       "      <td>2.272716</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.721855</td>\n",
       "      <td>1097.121691</td>\n",
       "      <td>7.328125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speed_avg    ttime_avg  Reliability  faciltype     nhs  strhnt_typ\n",
       "count  1054.000000  1054.000000  1054.000000     1054.0  1054.0       344.0\n",
       "mean     37.390528    84.592619     1.951307        2.0     1.0         1.0\n",
       "std      12.314169   144.149564     0.867956        0.0     0.0         0.0\n",
       "min       8.773512     0.630255     1.000000        2.0     1.0         1.0\n",
       "25%      28.262763     9.191569     1.308689        2.0     1.0         1.0\n",
       "50%      37.228934    29.798765     1.650016        2.0     1.0         1.0\n",
       "75%      47.586196    93.482095     2.272716        2.0     1.0         1.0\n",
       "max      61.721855  1097.121691     7.328125        2.0     1.0         1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[(final['faciltype'] == 2) & (final['summary_type'] == 'overnight')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b5eef7-f0d3-431d-958b-15a26d828595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HI-72 W', 'I-H1 W', 'I-H2 N', nan, 'I-H3 E', 'I-H201 W', 'I-H201',\n",
       "       'ALA MOANA BLVD', 'S NIMITZ HWY', 'N NIMITZ HWY', 'WILIKINA DR',\n",
       "       'KALIHI ST', 'HI-63 N', 'LIKELIKE HWY', 'KAMEHAMEHA HWY', 'HI-83',\n",
       "       'KAHEKILI HWY', 'PALI HWY', 'KALANIANAOLE HWY', 'HI-61 N',\n",
       "       'KAILUA RD', 'KAUMUALII HWY', 'KUHIO HWY', 'KAPIOLANI BLVD',\n",
       "       'WARD AVE', 'UNIVERSITY AVE', 'S BERETANIA ST', 'N BERETANIA ST',\n",
       "       'N KING ST', 'S KING ST', 'MOKAPU BLVD', 'HI-99 W',\n",
       "       'FARRINGTON HWY', 'HI-99 N', 'HI-99', 'S KAMEHAMEHA HWY',\n",
       "       'HI-311 N', 'MOKULELE HWY', 'S PUUNENE AVE', 'HI-380 N',\n",
       "       'S HIGH ST', 'HI-30', 'HONOAPIILANI HWY', 'HI-30 S', 'KULA HWY',\n",
       "       'HALEAKALA HWY', 'FORT BARRETTE AVE', 'FORT BARRETTE RD',\n",
       "       'MAKAKILO DR', 'FORT WEAVER RD', 'KUNIA RD', 'HI-750 N',\n",
       "       'WAAPA RD', 'RICE ST', 'KAPULE HWY', 'HI-51', 'NAWILIWILI RD',\n",
       "       'AHUKINI RD', 'HI-31 N', 'PIILANI HWY', 'HI-31', 'BISHOP ST',\n",
       "       'PUULOA RD', 'SAND ISLAND PKWY N', 'SAND ISLAND ACCESS RD',\n",
       "       'WAIALAE AVE', 'PUNCHBOWL ST', 'PENSACOLA ST',\n",
       "       'QUEEN KAAHUMANU HWY', 'KAWAIHAE RD', 'LINDSEY RD',\n",
       "       'HAWAII BELT RD', 'KAMEHAMEHA AVE', 'KANOELEHUA AVE',\n",
       "       'KUAKINI HWY', 'HI-11 N', 'HANA HWY', 'MAMALAHOA HWY',\n",
       "       'KANEOHE BAY DR', 'MOKAPU SADDLE RD', 'JOSEPH P LEONG HWY',\n",
       "       'ALAKEA ST', 'S VINEYARD BLVD', 'N VINEYARD BLVD', 'HALONA ST',\n",
       "       'PUNAHOU ST', 'I-H1 HOV LN E', 'HOBRON AVE', 'MAIN ST', 'HI-32',\n",
       "       'W KAAHUMANU AVE', 'HI-32 E', 'AKONI PULE HWY', 'I-H1 E', 'I-H2 S',\n",
       "       'I-H3 W', 'I-H201 E', 'HI-63 S', 'HI-61 S', 'HI-99 E', 'HI-99 S',\n",
       "       'HI-311 S', 'HI-380 S', 'HI-30 N', 'HI-750 S', 'HI-31 S', '18B',\n",
       "       'SAND ISLAND PKWY', 'OLOMEA ST', 'E KAAHUMANU AVE', 'HI-83 S',\n",
       "       'HI-50', 'HI-56 S', 'HI-56', 'HI-72 E', '9', 'HI-80', 'HI-37',\n",
       "       'HI-750', 'HI-58 S', 'HI-19 S', 'HI-19', 'HI-32 W', 'HI-83 N',\n",
       "       'HI-50 E', 'HI-56 N', 'MOKAPU RD', 'HI-58 N', 'HI-310 N',\n",
       "       'HI-19 N', '8A', '22'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['roadname'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cc7e71-d71b-490e-92ee-6ca193b438d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am_peak', 'midday', 'pm_peak', 'overnight', 'weekends', 'alltime'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['summary_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30f30bf-b566-4c59-b811-9b669ed0eec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>215.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.972803</td>\n",
       "      <td>39.541156</td>\n",
       "      <td>1.393298</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.630832</td>\n",
       "      <td>52.562649</td>\n",
       "      <td>0.378508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>24.210360</td>\n",
       "      <td>1.071236</td>\n",
       "      <td>1.088426</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.629844</td>\n",
       "      <td>12.704402</td>\n",
       "      <td>1.184629</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>53.486072</td>\n",
       "      <td>21.063124</td>\n",
       "      <td>1.245557</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57.324274</td>\n",
       "      <td>38.150065</td>\n",
       "      <td>1.422984</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.750000</td>\n",
       "      <td>403.399031</td>\n",
       "      <td>3.720399</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speed_avg   ttime_avg  Reliability  faciltype    nhs  strhnt_typ\n",
       "count  215.000000  215.000000   215.000000      215.0  215.0       210.0\n",
       "mean    52.972803   39.541156     1.393298        2.0    1.0         1.0\n",
       "std      5.630832   52.562649     0.378508        0.0    0.0         0.0\n",
       "min     24.210360    1.071236     1.088426        2.0    1.0         1.0\n",
       "25%     49.629844   12.704402     1.184629        2.0    1.0         1.0\n",
       "50%     53.486072   21.063124     1.245557        2.0    1.0         1.0\n",
       "75%     57.324274   38.150065     1.422984        2.0    1.0         1.0\n",
       "max     64.750000  403.399031     3.720399        2.0    1.0         1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = final[final['roadname'].str.contains('H1 |H2 |H3 |H201', na = False)]#.roadname.unique()\n",
    "\n",
    "a[a['summary_type'] == 'weekends'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c0ba94-eb86-4717-af75-27094984e53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>355.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>355.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.283522</td>\n",
       "      <td>51.418047</td>\n",
       "      <td>1.982226</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.510451</td>\n",
       "      <td>62.540273</td>\n",
       "      <td>1.356436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>23.373718</td>\n",
       "      <td>1.104771</td>\n",
       "      <td>1.077350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>43.579879</td>\n",
       "      <td>16.407539</td>\n",
       "      <td>1.188853</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49.997621</td>\n",
       "      <td>29.367119</td>\n",
       "      <td>1.394737</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.059991</td>\n",
       "      <td>62.113435</td>\n",
       "      <td>1.972959</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.611111</td>\n",
       "      <td>427.201092</td>\n",
       "      <td>9.400269</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speed_avg   ttime_avg  Reliability  faciltype    nhs  strhnt_typ\n",
       "count  355.000000  355.000000   355.000000      355.0  355.0       353.0\n",
       "mean    48.283522   51.418047     1.982226        2.0    1.0         1.0\n",
       "std      8.510451   62.540273     1.356436        0.0    0.0         0.0\n",
       "min     23.373718    1.104771     1.077350        2.0    1.0         1.0\n",
       "25%     43.579879   16.407539     1.188853        2.0    1.0         1.0\n",
       "50%     49.997621   29.367119     1.394737        2.0    1.0         1.0\n",
       "75%     55.059991   62.113435     1.972959        2.0    1.0         1.0\n",
       "max     61.611111  427.201092     9.400269        2.0    1.0         1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = final[final['roadname'].str.contains('H1 |H2 |H3 ', na = False)]#.roadname.unique()\n",
    "\n",
    "a[(a['summary_type'] == 'am_peak') | (a['summary_type'] == 'pm_peak')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee65a32d-0bad-4a0e-b020-5dd166b4243f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2199.000000</td>\n",
       "      <td>2199.000000</td>\n",
       "      <td>2199.000000</td>\n",
       "      <td>2199.000000</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.878456</td>\n",
       "      <td>93.616869</td>\n",
       "      <td>2.072163</td>\n",
       "      <td>1.959072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.433090</td>\n",
       "      <td>161.523941</td>\n",
       "      <td>0.850964</td>\n",
       "      <td>0.198168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.630255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.036821</td>\n",
       "      <td>10.475808</td>\n",
       "      <td>1.402384</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.939900</td>\n",
       "      <td>32.543537</td>\n",
       "      <td>1.910740</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.087424</td>\n",
       "      <td>104.049986</td>\n",
       "      <td>2.473830</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.721855</td>\n",
       "      <td>1502.809933</td>\n",
       "      <td>10.251989</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speed_avg    ttime_avg  Reliability    faciltype     nhs  strhnt_typ\n",
       "count  2199.000000  2199.000000  2199.000000  2199.000000  2199.0       689.0\n",
       "mean     33.878456    93.616869     2.072163     1.959072     1.0         1.0\n",
       "std      13.433090   161.523941     0.850964     0.198168     0.0         0.0\n",
       "min       6.000000     0.630255     1.000000     1.000000     1.0         1.0\n",
       "25%      23.036821    10.475808     1.402384     2.000000     1.0         1.0\n",
       "50%      32.939900    32.543537     1.910740     2.000000     1.0         1.0\n",
       "75%      45.087424   104.049986     2.473830     2.000000     1.0         1.0\n",
       "max      61.721855  1502.809933    10.251989     2.000000     1.0         1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[(final['summary_type'] == 'midday') | (final['summary_type'] == 'overnight')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ddc21f-b810-401f-b1a3-73bc1d4f41d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>summary_type</th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>Reliability_Type</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>Reliable</th>\n",
       "      <th>roadname</th>\n",
       "      <th>faciltype</th>\n",
       "      <th>nhs</th>\n",
       "      <th>strhnt_typ</th>\n",
       "      <th>isprimary</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126+04099</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>39.695578</td>\n",
       "      <td>39.613949</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.090194</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.78392 21.27838, -157.78417 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126+04100</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>44.484172</td>\n",
       "      <td>57.152976</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.099972</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.79424 21.27856, -157.79440 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126+04101</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>42.245109</td>\n",
       "      <td>70.743545</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>5.797780</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.80222 21.28137, -157.80244 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126+04102</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>39.585800</td>\n",
       "      <td>34.898510</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>4.661359</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.80985 21.28528, -157.81021 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>126+04103</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>am_peak</td>\n",
       "      <td>38.378998</td>\n",
       "      <td>8.152174</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>4.170886</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-157.81534 21.28955, -157.81555 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>126P04126</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>56.554112</td>\n",
       "      <td>49.444143</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.217400</td>\n",
       "      <td>True</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-158.02741 21.38658, -158.02902 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>126P04127</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>60.367163</td>\n",
       "      <td>23.938107</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.165150</td>\n",
       "      <td>True</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-158.07536 21.34449, -158.07599 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>126P04128</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>55.953358</td>\n",
       "      <td>39.656472</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.265600</td>\n",
       "      <td>True</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-158.08252 21.33715, -158.08298 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12284</th>\n",
       "      <td>126P04674</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>55.157687</td>\n",
       "      <td>19.148680</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>1.627808</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-158.08959 21.33224, -158.08982 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12381</th>\n",
       "      <td>126P04871</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>alltime</td>\n",
       "      <td>36.695652</td>\n",
       "      <td>2.429565</td>\n",
       "      <td>Truck_Traffic_95p_50p</td>\n",
       "      <td>3.811450</td>\n",
       "      <td>False</td>\n",
       "      <td>I-H1 HOV LN E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LINESTRING (-158.01532 21.39165, -158.01525 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tmc_code               data_origin summary_type  speed_avg  ttime_avg  \\\n",
       "1      126+04099  npmrds_from_inrix_trucks      am_peak  39.695578  39.613949   \n",
       "2      126+04100  npmrds_from_inrix_trucks      am_peak  44.484172  57.152976   \n",
       "3      126+04101  npmrds_from_inrix_trucks      am_peak  42.245109  70.743545   \n",
       "4      126+04102  npmrds_from_inrix_trucks      am_peak  39.585800  34.898510   \n",
       "5      126+04103  npmrds_from_inrix_trucks      am_peak  38.378998   8.152174   \n",
       "...          ...                       ...          ...        ...        ...   \n",
       "11982  126P04126  npmrds_from_inrix_trucks      alltime  56.554112  49.444143   \n",
       "11983  126P04127  npmrds_from_inrix_trucks      alltime  60.367163  23.938107   \n",
       "11984  126P04128  npmrds_from_inrix_trucks      alltime  55.953358  39.656472   \n",
       "12284  126P04674  npmrds_from_inrix_trucks      alltime  55.157687  19.148680   \n",
       "12381  126P04871  npmrds_from_inrix_trucks      alltime  36.695652   2.429565   \n",
       "\n",
       "            Reliability_Type  Reliability  Reliable       roadname  faciltype  \\\n",
       "1      Truck_Traffic_95p_50p     5.090194     False         I-H1 W        2.0   \n",
       "2      Truck_Traffic_95p_50p     5.099972     False         I-H1 W        2.0   \n",
       "3      Truck_Traffic_95p_50p     5.797780     False         I-H1 W        2.0   \n",
       "4      Truck_Traffic_95p_50p     4.661359     False         I-H1 W        2.0   \n",
       "5      Truck_Traffic_95p_50p     4.170886     False         I-H1 W        2.0   \n",
       "...                      ...          ...       ...            ...        ...   \n",
       "11982  Truck_Traffic_95p_50p     1.217400      True         I-H1 W        2.0   \n",
       "11983  Truck_Traffic_95p_50p     1.165150      True         I-H1 W        2.0   \n",
       "11984  Truck_Traffic_95p_50p     1.265600      True         I-H1 W        2.0   \n",
       "12284  Truck_Traffic_95p_50p     1.627808     False         I-H1 W        2.0   \n",
       "12381  Truck_Traffic_95p_50p     3.811450     False  I-H1 HOV LN E        2.0   \n",
       "\n",
       "       nhs  strhnt_typ isprimary  \\\n",
       "1      1.0         1.0         1   \n",
       "2      1.0         1.0         1   \n",
       "3      1.0         1.0         1   \n",
       "4      1.0         1.0         1   \n",
       "5      1.0         1.0         1   \n",
       "...    ...         ...       ...   \n",
       "11982  1.0         1.0         1   \n",
       "11983  1.0         1.0         1   \n",
       "11984  1.0         1.0         1   \n",
       "12284  1.0         1.0         1   \n",
       "12381  1.0         1.0         1   \n",
       "\n",
       "                                                geometry  \n",
       "1      LINESTRING (-157.78392 21.27838, -157.78417 21...  \n",
       "2      LINESTRING (-157.79424 21.27856, -157.79440 21...  \n",
       "3      LINESTRING (-157.80222 21.28137, -157.80244 21...  \n",
       "4      LINESTRING (-157.80985 21.28528, -157.81021 21...  \n",
       "5      LINESTRING (-157.81534 21.28955, -157.81555 21...  \n",
       "...                                                  ...  \n",
       "11982  LINESTRING (-158.02741 21.38658, -158.02902 21...  \n",
       "11983  LINESTRING (-158.07536 21.34449, -158.07599 21...  \n",
       "11984  LINESTRING (-158.08252 21.33715, -158.08298 21...  \n",
       "12284  LINESTRING (-158.08959 21.33224, -158.08982 21...  \n",
       "12381  LINESTRING (-158.01532 21.39165, -158.01525 21...  \n",
       "\n",
       "[789 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final['roadname'].str.contains('H1 ', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8801fb-8548-4c1e-a6e7-be2aa015796a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
